# ğŸ¤– AI Agent Test Repository

A comprehensive testing suite for evaluating AI coding agents across different skill levels and domains.

## ğŸ¯ Overview

This repository contains **5 progressive test scenarios** designed to evaluate AI agents on:
- **Basic Implementation Skills** (utility functions, algorithms)
- **UI/Frontend Development** (responsive design, user interfaces)  
- **API Integration** (data fetching, error handling, async operations)
- **Complex Logic** (advanced algorithms, system design)
- **Full-Stack Development** (end-to-end application building)

## ğŸš€ Quick Start

### For AI Agents Being Tested

1. **Clone this repository**
   ```bash
   git clone <repo-url>
   cd ai-agent-test-repo
   npm install
   ```

2. **Choose your test scenario**
   ```bash
   cd tests/01-simple-implementation  # Start with basics
   # OR
   cd tests/02-ui-dashboard          # Try UI development
   # OR choose any other test
   ```

3. **Read the requirements**
   ```bash
   cat TASK.md                       # Quick overview
   cat README.md                     # Detailed requirements
   ```

4. **Implement your solution**
   - Work in the `initial/` directory
   - Follow the requirements in TASK.md
   - Test your implementation regularly

5. **Validate your work**
   ```bash
   npm test                          # Run validation tests
   ```

### For Evaluators

1. **Run comprehensive evaluation**
   ```bash
   node evaluation/evaluator.js "Agent Name"
   ```

2. **View results**
   ```bash
   ls evaluation/results/            # See all evaluation results
   ```

## ğŸ“‹ Test Scenarios

| Test | Difficulty | Time | Skills |
|------|------------|------|--------|
| **01-Simple Implementation** | â­ | 15-20 min | Basic algorithms, utility functions |
| **02-UI Dashboard** | â­â­ | 25-35 min | HTML/CSS, responsive design, JavaScript |
| **03-API Integration** | â­â­â­ | 30-40 min | REST APIs, async/await, error handling |
| **04-Complex Logic** | â­â­â­â­ | 40-50 min | Advanced algorithms, data structures |
| **05-Full Stack** | â­â­â­â­â­ | 60-75 min | End-to-end development, architecture |

## ğŸ¯ Evaluation Criteria

Each test is evaluated on:
- **Functionality** (40%): Does it work as specified?
- **Code Quality** (25%): Clean, maintainable, well-structured code?
- **Problem Solving** (20%): Good approach to solving the challenges?
- **Edge Cases** (15%): Handles errors and edge cases properly?

## ğŸ“Š Scoring System

- **A (90-100%)**: Exceptional implementation, exceeds expectations
- **B (80-89%)**: Good implementation with minor issues
- **C (70-79%)**: Adequate implementation, meets basic requirements
- **D (60-69%)**: Below expectations, significant issues
- **F (0-59%)**: Major problems or non-functional

## ğŸ”§ Repository Structure

```
ai-agent-test-repo/
â”œâ”€â”€ tests/                    # Test scenarios
â”‚   â”œâ”€â”€ 01-simple-implementation/
â”‚   â”‚   â”œâ”€â”€ initial/         # Starting code (work here)
â”‚   â”‚   â”œâ”€â”€ solution/        # Reference solution
â”‚   â”‚   â”œâ”€â”€ tests/          # Validation tests
â”‚   â”‚   â”œâ”€â”€ TASK.md         # Quick overview
â”‚   â”‚   â””â”€â”€ README.md       # Detailed requirements
â”‚   â”œâ”€â”€ 02-ui-dashboard/
â”‚   â”œâ”€â”€ 03-api-integration/
â”‚   â”œâ”€â”€ 04-complex-logic/
â”‚   â””â”€â”€ 05-full-stack/
â”œâ”€â”€ shared/                   # Common utilities
â”œâ”€â”€ evaluation/              # Evaluation system
â”‚   â”œâ”€â”€ evaluator.js        # Main evaluation script
â”‚   â””â”€â”€ results/            # Evaluation results
â””â”€â”€ README.md               # This file
```

## ğŸ® Usage Examples

### Test a Single Scenario
```bash
cd tests/01-simple-implementation
npm install
npm test                    # Validate your implementation
```

### Run Full Evaluation
```bash
node evaluation/evaluator.js "Claude"
node evaluation/evaluator.js "GPT-4"
node evaluation/evaluator.js "Gemini"
```

### Compare Results
```bash
# Results are saved in evaluation/results/
# Compare JSON files to see performance differences
```

## ğŸ† Success Tips for AI Agents

1. **Read requirements carefully** - Understanding the task is half the battle
2. **Start simple** - Get basic functionality working first
3. **Test frequently** - Run tests early and often
4. **Handle edge cases** - Consider null values, empty inputs, error conditions
5. **Write clean code** - Use good naming, structure, and comments
6. **Time management** - Don't spend too long on any single part

## ğŸ” For Researchers & Evaluators

This test suite is designed to provide:
- **Objective measurements** of AI coding capabilities
- **Progressive difficulty** to identify skill levels
- **Diverse domains** to test different types of intelligence
- **Reproducible results** for fair comparisons
- **Detailed feedback** for improvement insights

## ğŸ“ˆ Contributing

To add new test scenarios:
1. Create a new directory in `tests/`
2. Follow the established structure (initial/, solution/, tests/, TASK.md, README.md)
3. Update this README with the new test information
4. Test thoroughly before submitting

## ğŸ“„ License

MIT License - Feel free to use this for AI agent evaluation and research!

---

**Happy Testing!** ğŸš€ May the best AI agent win! ğŸ†
